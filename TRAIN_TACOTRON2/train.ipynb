{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_path = \"/home/ubuntu/titovox/DATA_FOR_TRAINING/\"\n",
        "original_wavs_path = f\"{training_path}data_pp/\"\n",
        "original_txt_path = f\"{training_path}metadata.txt\"\n",
        "tacotron_path = \"/home/ubuntu/titovox/TRAIN_TACOTRON2/tacotron2/\"\n",
        "new_model_path = \"/home/ubuntu/titovox/TRAIN_TACOTRON2/titovox/\"\n",
        "\n",
        "\n",
        "wavs_path = tacotron_path + \"wavs/\"\n",
        "txt_path = tacotron_path + \"filelists/list.txt\"\n",
        "\n",
        "model_filename = \"titovox\"\n",
        "\n",
        "FP16 = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports \n",
        "\n",
        "import sys\n",
        "sys.path.append('tacotron2')\n",
        "sys.path.append('tacotron2/hifi-gan')\n",
        "\n",
        "from numba import cuda \n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "from os.path import exists\n",
        "import IPython.display as ipd\n",
        "import json\n",
        "from layers import TacotronSTFT\n",
        "from audio_processing import griffin_lim\n",
        "from env import AttrDict\n",
        "from meldataset import mel_spectrogram, MAX_WAV_VALUE\n",
        "from models import Generator\n",
        "from denoiser import Denoiser\n",
        "import resampy\n",
        "import scipy.signal\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import math\n",
        "from numpy import finfo\n",
        "import torch\n",
        "from distributed import apply_gradient_allreduce\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import DataLoader\n",
        "from model import Tacotron2\n",
        "from data_utils import TextMelLoader, TextMelCollate\n",
        "from loss_function import Tacotron2Loss\n",
        "from logger import Tacotron2Logger\n",
        "from hparams import create_hparams\n",
        "import random\n",
        "import numpy as np\n",
        "import layers\n",
        "from utils import load_wav_to_torch, load_filepaths_and_text\n",
        "from text import text_to_sequence\n",
        "from math import e\n",
        "from tqdm.notebook import tqdm # Modern Notebook TQDM\n",
        "from distutils.dir_util import copy_tree\n",
        "import matplotlib.pylab as plt\n",
        "import re\n",
        "import num2words\n",
        "import sys\n",
        "import os\n",
        "import os.path\n",
        "from pathlib import Path\n",
        "from itertools import chain\n",
        "from glob import glob\n",
        "import zipfile\n",
        "import os\n",
        "import wave\n",
        "import shutil\n",
        "import datetime\n",
        "\n",
        "os.chdir(tacotron_path)\n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6zQ-DWo4lcs",
        "outputId": "d2d245c9-0b70-4335-a7c8-ba3abe8865e9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_hifigan(MODEL_ID, conf_name):\n",
        "    # Download HiFi-GAN\n",
        "    hifigan_pretrained_model = 'hifimodel_' + conf_name\n",
        "    #gdown.download(d+MODEL_ID, hifigan_pretrained_model, quiet=False)\n",
        "\n",
        "    if MODEL_ID == \"universal\":\n",
        "      !wget \"https://github.com/johnpaulbin/tacotron2/releases/download/Main/g_02500000\" -O $hifigan_pretrained_model\n",
        "    else:\n",
        "      !gdown --id \"$MODEL_ID\" -O $hifigan_pretrained_model\n",
        "\n",
        "    # Load HiFi-GAN\n",
        "    conf = os.path.join(\"hifi-gan\", conf_name + \".json\")\n",
        "    with open(conf) as f:\n",
        "        json_config = json.loads(f.read())\n",
        "    h = AttrDict(json_config)\n",
        "    torch.manual_seed(h.seed)\n",
        "    hifigan = Generator(h).to(torch.device(\"cuda\"))\n",
        "    state_dict_g = torch.load(hifigan_pretrained_model, map_location=torch.device(\"cuda\"))\n",
        "    hifigan.load_state_dict(state_dict_g[\"generator\"])\n",
        "    hifigan.eval()\n",
        "    hifigan.remove_weight_norm()\n",
        "    denoiser = Denoiser(hifigan, mode=\"normal\")\n",
        "    return hifigan, h, denoiser\n",
        " \n",
        "# Download character HiFi-GAN\n",
        "hifigan, h, denoiser = get_hifigan(\"universal\", \"config_v1\")\n",
        "# Download super-resolution HiFi-GAN\n",
        "hifigan_sr, h2, denoiser_sr = get_hifigan(\"14fOprFAIlCQkVRxsfInhEPG0n-xN4QOa\", \"config_32k\")\n",
        "\n",
        "\n",
        "def download_from_google_drive(file_id, file_name):\n",
        "  # download a file from the Google Drive link\n",
        "  !rm -f ./cookie\n",
        "  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id={file_id}\" > /dev/null\n",
        "  confirm_text = !awk '/download/ {print $NF}' ./cookie\n",
        "  confirm_text = confirm_text[0]\n",
        "  !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm={confirm_text}&id={file_id}\" -o {file_name}\n",
        "\n",
        "def create_mels():\n",
        "    print(\"Generating Mels\")\n",
        "    stft = layers.TacotronSTFT(\n",
        "                hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
        "                hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
        "                hparams.mel_fmax)\n",
        "    def save_mel(filename):\n",
        "        audio, sampling_rate = load_wav_to_torch(filename)\n",
        "        if sampling_rate != stft.sampling_rate:\n",
        "            raise ValueError(\"{} {} SR does not match the objective {} SR\".format(filename, \n",
        "                sampling_rate, stft.sampling_rate))\n",
        "        audio_norm = audio / hparams.max_wav_value\n",
        "        audio_norm = audio_norm.unsqueeze(0)\n",
        "        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "        melspec = stft.mel_spectrogram(audio_norm)\n",
        "        melspec = torch.squeeze(melspec, 0).cpu().numpy()\n",
        "        np.save(filename.replace('.wav', ''), melspec)\n",
        "\n",
        "    import glob\n",
        "    wavs = glob.glob('wavs/*.wav')\n",
        "    for i in tqdm(wavs):\n",
        "        save_mel(i)\n",
        "\n",
        "\n",
        "def reduce_tensor(tensor, n_gpus):\n",
        "    rt = tensor.clone()\n",
        "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
        "    rt /= n_gpus\n",
        "    return rt\n",
        "\n",
        "\n",
        "def init_distributed(hparams, n_gpus, rank, group_name):\n",
        "    assert torch.cuda.is_available(), \"Distributed mode requires CUDA.\"\n",
        "    print(\"Initializing Distributed\")\n",
        "\n",
        "    # Set cuda device so everything is done on the right GPU.\n",
        "    torch.cuda.set_device(rank % torch.cuda.device_count())\n",
        "\n",
        "    # Initialize distributed communication\n",
        "    dist.init_process_group(\n",
        "        backend=hparams.dist_backend, init_method=hparams.dist_url,\n",
        "        world_size=n_gpus, rank=rank, group_name=group_name)\n",
        "\n",
        "    print(\"Done initializing distributed\")\n",
        "\n",
        "\n",
        "def prepare_dataloaders(hparams):\n",
        "    # Get data, data loaders and collate function ready\n",
        "    trainset = TextMelLoader(hparams.training_files, hparams)\n",
        "    valset = TextMelLoader(hparams.validation_files, hparams)\n",
        "    collate_fn = TextMelCollate(hparams.n_frames_per_step)\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        train_sampler = DistributedSampler(trainset)\n",
        "        shuffle = False\n",
        "    else:\n",
        "        train_sampler = None\n",
        "        shuffle = True\n",
        "\n",
        "    train_loader = DataLoader(trainset, num_workers=1, shuffle=shuffle,\n",
        "                              sampler=train_sampler,\n",
        "                              batch_size=hparams.batch_size, pin_memory=False,\n",
        "                              drop_last=True, collate_fn=collate_fn)\n",
        "    return train_loader, valset, collate_fn\n",
        "\n",
        "\n",
        "def prepare_directories_and_logger(output_directory, log_directory, rank):\n",
        "    if rank == 0:\n",
        "        if not os.path.isdir(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "            os.chmod(output_directory, 0o775)\n",
        "        logger = Tacotron2Logger(os.path.join(output_directory, log_directory))\n",
        "    else:\n",
        "        logger = None\n",
        "    return logger\n",
        "\n",
        "\n",
        "def load_model(hparams):\n",
        "    model = Tacotron2(hparams).cuda()\n",
        "    if hparams.fp16_run:\n",
        "        model.decoder.attention_layer.score_mask_value = finfo('float16').min\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        model = apply_gradient_allreduce(model)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def warm_start_model(checkpoint_path, model, ignore_layers):\n",
        "    assert os.path.isfile(checkpoint_path)\n",
        "    print(\"Warm starting model from checkpoint '{}'\".format(checkpoint_path))\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model_dict = checkpoint_dict['state_dict']\n",
        "    if len(ignore_layers) > 0:\n",
        "        model_dict = {k: v for k, v in model_dict.items()\n",
        "                      if k not in ignore_layers}\n",
        "        dummy_dict = model.state_dict()\n",
        "        dummy_dict.update(model_dict)\n",
        "        model_dict = dummy_dict\n",
        "    model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "    assert os.path.isfile(checkpoint_path)\n",
        "    print(\"Loading checkpoint '{}'\".format(checkpoint_path))\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint_dict['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
        "    learning_rate = checkpoint_dict['learning_rate']\n",
        "    iteration = checkpoint_dict['iteration']\n",
        "    print(\"Loaded checkpoint '{}' from iteration {}\" .format(\n",
        "        checkpoint_path, iteration))\n",
        "    return model, optimizer, learning_rate, iteration\n",
        "\n",
        "def save_checkpoint(model, optimizer, learning_rate, iteration, filepath):\n",
        "    print(\"Saving model and optimizer state at iteration {} to {}\".format(\n",
        "        iteration, filepath))\n",
        "    try:\n",
        "        torch.save({'iteration': iteration,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'learning_rate': learning_rate}, filepath)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"interrupt received while saving, waiting for save to complete.\")\n",
        "        torch.save({'iteration': iteration,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(),'learning_rate': learning_rate}, filepath)\n",
        "    print(\"Model Saved\")\n",
        "\n",
        "def plot_alignment(alignment, info=None):\n",
        "    %matplotlib inline\n",
        "    fig, ax = plt.subplots(figsize=(int(alignment_graph_width/100), int(alignment_graph_height/100)))\n",
        "    im = ax.imshow(alignment, cmap='inferno', aspect='auto', origin='lower',\n",
        "                   interpolation='none')\n",
        "    ax.autoscale(enable=True, axis=\"y\", tight=True)\n",
        "    fig.colorbar(im, ax=ax)\n",
        "    xlabel = 'Decoder timestep'\n",
        "    if info is not None:\n",
        "        xlabel += '\\n\\n' + info\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel('Encoder timestep')\n",
        "    plt.tight_layout()\n",
        "    fig.canvas.draw()\n",
        "    plt.show()\n",
        "\n",
        "def validate(model, criterion, valset, iteration, batch_size, n_gpus,\n",
        "             collate_fn, logger, distributed_run, rank, epoch, start_eposh, learning_rate, sample_interbal, save_audio = False, audio_path = None):\n",
        "    \"\"\"Handles all the validation scoring and printing\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_sampler = DistributedSampler(valset) if distributed_run else None\n",
        "        val_loader = DataLoader(valset, sampler=val_sampler, num_workers=1,\n",
        "                                shuffle=False, batch_size=batch_size,\n",
        "                                pin_memory=False, collate_fn=collate_fn)\n",
        "\n",
        "        val_loss = 0.0\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            x, y = model.parse_batch(batch)\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            if distributed_run:\n",
        "                reduced_val_loss = reduce_tensor(loss.data, n_gpus).item()\n",
        "            else:\n",
        "                reduced_val_loss = loss.item()\n",
        "            val_loss += reduced_val_loss\n",
        "        val_loss = val_loss / (i + 1)\n",
        "\n",
        "    model.train()\n",
        "    if rank == 0:\n",
        "        print(\"Epoch: {} Validation loss {}: {:9f}  Time: {:.1f}m LR: {:.6f}\".format(epoch, iteration, val_loss,(time.perf_counter()-start_eposh)/60, learning_rate))\n",
        "        logger.log_validation(val_loss, model, y, y_pred, iteration)\n",
        "        if hparams.show_alignments:\n",
        "            %matplotlib inline\n",
        "            _, mel_outputs, gate_outputs, alignments = y_pred\n",
        "            idx = random.randint(0, alignments.size(0) - 1)\n",
        "            plot_alignment(alignments[idx].data.cpu().numpy().T)\n",
        "\n",
        "    dv = epoch/sample_interbal\n",
        "    if dv.is_integer():\n",
        "      print(f\"Generación de muestras... \\n{sampletext}\")\n",
        "      for i in [x for x in sampletext.split(\"\\n\") if len(x)]:\n",
        "          if i[-1] != \";\": i=i+\";\" \n",
        "          with torch.no_grad():\n",
        "              sequence = np.array(text_to_sequence(i, ['basic_cleaners']))[None, :]\n",
        "              sequence = torch.autograd.Variable(torch.from_numpy(sequence)).cuda().long()\n",
        "              mel_outputs, mel_outputs_postnet, _, alignments = model.inference(sequence)\n",
        "              y_g_hat = hifigan(mel_outputs_postnet.float())\n",
        "              audio = y_g_hat.squeeze()\n",
        "              audio = audio * MAX_WAV_VALUE\n",
        "              audio_denoised = denoiser(audio.view(1, -1), strength=35)[:, 0]\n",
        " \n",
        "              # Resample to 32k\n",
        "              audio_denoised = audio_denoised.cpu().numpy().reshape(-1)\n",
        " \n",
        "              normalize = (MAX_WAV_VALUE / np.max(np.abs(audio_denoised))) ** 0.9\n",
        "              audio_denoised = audio_denoised * normalize\n",
        "              wave = resampy.resample(\n",
        "                  audio_denoised,\n",
        "                  h.sampling_rate,\n",
        "                  h2.sampling_rate,\n",
        "                  filter=\"sinc_window\",\n",
        "                  window=scipy.signal.windows.hann,\n",
        "                  num_zeros=8,\n",
        "              )\n",
        "              wave_out = wave.astype(np.int16)\n",
        " \n",
        "              # HiFi-GAN super-resolution\n",
        "              wave = wave / MAX_WAV_VALUE\n",
        "              wave = torch.FloatTensor(wave).to(torch.device(\"cuda\"))\n",
        "              new_mel = mel_spectrogram(\n",
        "                  wave.unsqueeze(0),\n",
        "                  h2.n_fft,\n",
        "                  h2.num_mels,\n",
        "                  h2.sampling_rate,\n",
        "                  h2.hop_size,\n",
        "                  h2.win_size,\n",
        "                  h2.fmin,\n",
        "                  h2.fmax,\n",
        "              )\n",
        "              y_g_hat2 = hifigan_sr(new_mel)\n",
        "              audio2 = y_g_hat2.squeeze()\n",
        "              audio2 = audio2 * MAX_WAV_VALUE\n",
        "              audio2_denoised = denoiser(audio2.view(1, -1), strength=35)[:, 0]\n",
        "                  \n",
        "              # High-pass filter, mixing and denormalizing\n",
        "              audio2_denoised = audio2_denoised.cpu().numpy().reshape(-1)\n",
        "              b = scipy.signal.firwin(\n",
        "                  101, cutoff=10500, fs=h2.sampling_rate, pass_zero=False\n",
        "              )\n",
        "              y = scipy.signal.lfilter(b, [1.0], audio2_denoised)\n",
        "              y *= 0\n",
        "              y_out = y.astype(np.int16)\n",
        "              y_padded = np.zeros(wave_out.shape)\n",
        "              y_padded[: y_out.shape[0]] = y_out\n",
        "              sr_mix = wave_out + y_padded\n",
        "              sr_mix = sr_mix / normalize\n",
        "\n",
        "              print(\"\")\n",
        "              ipd.display(ipd.Audio(sr_mix.astype(np.int16), rate=h2.sampling_rate))\n",
        "              if save_audio:\n",
        "                if not os.path.isdir(audio_path+\"/audio samples\"):\n",
        "                  !os.makedirs(audio_path+\"/audio samples\")\n",
        "                scipy.io.wavfile.write(audio_path+\"/audio samples/_\"+epoch+\"test.wav\", h2.sampling_rate, sr_mix.astype(np.int16))\n",
        "                wav2mp3(audio_path+\"/audio samples/_\"+epoch+\"test.wav\")\n",
        "\n",
        "def train(output_directory, log_directory, checkpoint_path, warm_start, n_gpus,\n",
        "          rank, group_name, hparams, log_directory2):\n",
        "    \"\"\"Training and validation logging results to tensorboard and stdout\n",
        "\n",
        "    Params\n",
        "    ------\n",
        "    output_directory (string): directory to save checkpoints\n",
        "    log_directory (string) directory to save tensorboard logs\n",
        "    checkpoint_path(string): checkpoint path\n",
        "    n_gpus (int): number of gpus\n",
        "    rank (int): rank of current gpu\n",
        "    hparams (object): comma separated list of \"name=value\" pairs.\n",
        "    \"\"\"\n",
        "    if hparams.distributed_run:\n",
        "        init_distributed(hparams, n_gpus, rank, group_name)\n",
        "\n",
        "    torch.manual_seed(hparams.seed)\n",
        "    torch.cuda.manual_seed(hparams.seed)\n",
        "\n",
        "    model = load_model(hparams)\n",
        "    learning_rate = hparams.learning_rate\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                                 weight_decay=hparams.weight_decay)\n",
        "\n",
        "    if hparams.fp16_run:\n",
        "        from apex import amp\n",
        "        model, optimizer = amp.initialize(\n",
        "            model, optimizer, opt_level='O2')\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        model = apply_gradient_allreduce(model)\n",
        "\n",
        "    criterion = Tacotron2Loss()\n",
        "\n",
        "    logger = prepare_directories_and_logger(\n",
        "        output_directory, log_directory, rank)\n",
        "\n",
        "    train_loader, valset, collate_fn = prepare_dataloaders(hparams)\n",
        "\n",
        "    # Load checkpoint if one exists\n",
        "    iteration = 0\n",
        "    svcount = 0\n",
        "    epoch_offset = 0\n",
        "    if checkpoint_path is not None and os.path.isfile(checkpoint_path):\n",
        "        if warm_start:\n",
        "            model = warm_start_model(\n",
        "                checkpoint_path, model, hparams.ignore_layers)\n",
        "        else:\n",
        "            model, optimizer, _learning_rate, iteration = load_checkpoint(\n",
        "                checkpoint_path, model, optimizer)\n",
        "            if hparams.use_saved_learning_rate:\n",
        "                learning_rate = _learning_rate\n",
        "            iteration += 1  # next iteration is iteration + 1\n",
        "            epoch_offset = max(0, int(iteration / len(train_loader)))\n",
        "    elif model_base != \"Ninguno\":\n",
        "        raise FileNotFoundError(f\"Modelo '{checkpoint_path}' no se encuentra.\")\n",
        "    \n",
        "    start_eposh = time.perf_counter()\n",
        "    learning_rate = 0.0\n",
        "    model.train()\n",
        "    is_overflow = False\n",
        "    # ================ MAIN TRAINNIG LOOP! ===================\n",
        "    for epoch in tqdm(range(epoch_offset, hparams.epochs)):\n",
        "        print(\"\\nStarting Epoch: {} Iteration: {}\".format(epoch, iteration))\n",
        "        start_eposh = time.perf_counter() # eposh is russian, not a typo\n",
        "        for i, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "            start = time.perf_counter()\n",
        "            if iteration < hparams.decay_start: learning_rate = hparams.A_\n",
        "            else: iteration_adjusted = iteration - hparams.decay_start; learning_rate = (hparams.A_*(e**(-iteration_adjusted/hparams.B_))) + hparams.C_\n",
        "            learning_rate = max(hparams.min_learning_rate, learning_rate) # output the largest number\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = learning_rate\n",
        "\n",
        "            model.zero_grad()\n",
        "            x, y = model.parse_batch(batch)\n",
        "            y_pred = model(x)\n",
        "\n",
        "            loss = criterion(y_pred, y)\n",
        "            if hparams.distributed_run:\n",
        "                reduced_loss = reduce_tensor(loss.data, n_gpus).item()\n",
        "            else:\n",
        "                reduced_loss = loss.item()\n",
        "            if hparams.fp16_run:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            if hparams.fp16_run:\n",
        "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                    amp.master_params(optimizer), hparams.grad_clip_thresh)\n",
        "                is_overflow = math.isnan(grad_norm)\n",
        "            else:\n",
        "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                    model.parameters(), hparams.grad_clip_thresh)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if not is_overflow and rank == 0:\n",
        "                duration = time.perf_counter() - start\n",
        "                logger.log_training(\n",
        "                    reduced_loss, grad_norm, learning_rate, duration, iteration)\n",
        "                #print(\"Batch {} loss {:.6f} Grad Norm {:.6f} Time {:.6f}\".format(iteration, reduced_loss, grad_norm, duration), end='\\r', flush=True)\n",
        "\n",
        "            iteration += 1\n",
        "        validate(model, criterion, valset, iteration,\n",
        "                 hparams.batch_size, n_gpus, collate_fn, logger,\n",
        "                 hparams.distributed_run, rank, epoch, start_eposh, learning_rate, sample_interbal, save_audio, output_directory)\n",
        "        svcount += 1\n",
        "        if svcount == saving_interval:\n",
        "          svcount = 0\n",
        "          save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path)\n",
        "        if log_directory2 != None:\n",
        "            copy_tree(log_directory, log_directory2)\n",
        "\n",
        "\n",
        "def wav2mp3(path_to_file):\n",
        "    final_audio = AudioSegment.from_wav(file=path_to_file)\n",
        "    path_to_file = path_to_file.replace(\".wav\",\".mp3\")\n",
        "    final_audio.export(path_to_file, format=\"mp3\")\n",
        "    return path_to_file\n",
        "\n",
        "def check_dataset(hparams):\n",
        "    from utils import load_wav_to_torch, load_filepaths_and_text\n",
        "    import os\n",
        "    import numpy as np\n",
        "    def check_arr(filelist_arr):\n",
        "        for i, file in enumerate(filelist_arr):\n",
        "            if len(file) > 2:\n",
        "                print(\"|\".join(file), \" tiene múltiplos '|', esto puede no ser un error.\\n\")\n",
        "            if hparams.load_mel_from_disk and '.wav' in file[0]:\n",
        "                print(\"\\033[31m\\033[1m[AVISO]\", file[0], \" en la lista de archivos mientras que lo esperado era .npy\\n\")\n",
        "            else:\n",
        "                if not hparams.load_mel_from_disk and '.npy' in file[0]:\n",
        "                    print(\"\\033[31m\\033[1m[AVISO]\", file[0], \" en la lista de archivos mientras que lo esperado era .wav\\n\")\n",
        "            if (not os.path.exists(file[0])):\n",
        "                raise FileNotFoundError(f\"\\'{'|'.join(file)}\\' no existe. Compruebe su transcripción y sus audios.\")\n",
        "            if len(file[1]) < 3:\n",
        "                print(f\"\\033[34m\\033[1m[info]{'|'.join(file)} sin texto o demasiado corto.\\n\")\n",
        "            if not ((file[1].strip())[-1] in r\"!?,.;:\"):\n",
        "                print(f\"\\033[34m\\033[1m[info]{'|'.join(file)} no tiene puntuación final.\\n\")\n",
        "            mel_length = 1\n",
        "            if hparams.load_mel_from_disk and '.npy' in file[0]:\n",
        "                melspec = torch.from_numpy(np.load(file[0], allow_pickle=True))\n",
        "                mel_length = melspec.shape[1]\n",
        "            if mel_length == 0:\n",
        "                print(f\"\\033[33m\\033[1m[AVISO]{'|'.join(file)} has 0s duration.\\n\")\n",
        "\n",
        "    print(\"Comprobación de los archivos de formación\")\n",
        "    audiopaths_and_text = load_filepaths_and_text(hparams.training_files) # get split lines from training_files text file.\n",
        "    check_arr(audiopaths_and_text)\n",
        "    print(\"Comprobación de los archivos de formación\")\n",
        "    audiopaths_and_text = load_filepaths_and_text(hparams.validation_files) # get split lines from validation_files text file.\n",
        "    check_arr(audiopaths_and_text)\n",
        "    print(\"\\033[32m\\033[1mComprobación realizada\")\n",
        "n_gpus=1\n",
        "rank=0\n",
        "group_name=None\n",
        "\n",
        "# ---- DEFAULT PARAMETERS DEFINED HERE ----\n",
        "hparams = create_hparams()\n",
        "model_filename = 'current_model'\n",
        "hparams.training_files = f\"{tacotron_path}filelists/clipper_train_filelist.txt\"\n",
        "hparams.validation_files = f\"{tacotron_path}filelists/clipper_val_filelist.txt\"\n",
        "#hparams.use_mmi=True,          # not used in this notebook\n",
        "#hparams.use_gaf=True,          # not used in this notebook\n",
        "#hparams.max_gaf=0.5,           # not used in this notebook\n",
        "#hparams.drop_frame_rate = 0.2  # not used in this notebook\n",
        "hparams.fp16_run = FP16\n",
        "hparams.p_attention_dropout=0.1\n",
        "hparams.p_decoder_dropout=0.1\n",
        "hparams.decay_start = 15000\n",
        "hparams.A_ = 5e-4\n",
        "hparams.B_ = 8000\n",
        "hparams.C_ = 0\n",
        "hparams.min_learning_rate = 1e-5\n",
        "generate_mels = True\n",
        "hparams.show_alignments = True\n",
        "alignment_graph_height = 600\n",
        "alignment_graph_width = 1000\n",
        "hparams.batch_size = 32\n",
        "hparams.load_mel_from_disk = True\n",
        "hparams.epochs = 10000\n",
        "hparams.sampling_rate = 22050\n",
        "hparams.max_decoder_steps = 3000 # Max Duration\n",
        "hparams.gate_threshold = 0.5 # Model must be 50% sure the clip is over before ending generation\n",
        "torch.backends.cudnn.enabled = hparams.cudnn_enabled\n",
        "torch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n",
        "output_directory = '{tacotron_path}outdir' # Location to save Checkpoints\n",
        "log_directory = '{tacotron_path}logs' # Location to save Log files locally\n",
        "log_directory2 = '{tacotron_path}logs' # Location to copy log files (done at the end of each epoch to cut down on I/O)\n",
        "checkpoint_path = output_directory+(r'/')+model_filename\n",
        "sample_interbal = 5\n",
        "save_audio = False\n",
        "# ---- Replace .wav with .npy in filelists ----\n",
        "!sed -i -- 's,.wav|,.npy|,g' filelists/*.txt\n",
        "!sed -i -- 's,.wav|,.npy|,g' {hparams.training_files}\n",
        "!sed -i -- 's,.wav|,.npy|,g' {hparams.validation_files}\n",
        "# ---- Replace .wav with .npy in filelists ----\n",
        "\n",
        "# %cd /content/tacotron2\n",
        "\n",
        "data_path = 'wavs'\n",
        "!mkdir {data_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j18Q4ku-oL0J",
        "outputId": "2bbee456-94f3-4151-91bc-522746dbeedf"
      },
      "outputs": [],
      "source": [
        "# Copy audios \n",
        "# Copy wavs_path to tacotron_path/wavs (forcing overwrite)\n",
        "import shutil \n",
        "\n",
        "# if wavs_path is empty, copy wavs_path to tacotron_path/wavs\n",
        "if not os.listdir(wavs_path):\n",
        "  shutil.copytree(original_wavs_path, wavs_path, dirs_exist_ok=True)\n",
        "\n",
        "#### ¿Habilitar procesamiento de audios?\n",
        "#Recuerda que estos deberán tener un formato compatible, es decir, frecuencia de muestreo 22050, 16 bit, mono. Si no tienes los audios con este formato, activa esta casilla para hacer la conversión, a parte de normalización y eliminación de silencios.\n",
        "audio_processing = True \n",
        "\n",
        "# Set current directory to wavs_path\n",
        "os.chdir(wavs_path)\n",
        "\n",
        "tempwav_path = wavs_path + \"tempwav/\"\n",
        "\n",
        "# if audios.sh exists, delete it\n",
        "if os.path.exists(\"audios.sh\"):\n",
        "  os.remove(\"audios.sh\")\n",
        "with open('audios.sh', 'w') as rsh:\n",
        "    rsh.write(f'''\\\n",
        "for file in {wavs_path}*.wav\n",
        "do\n",
        "    ffmpeg -y -i \"$file\" -ar 22050 {tempwav_path}srtmp.wav -loglevel error\n",
        "    sox {tempwav_path}srtmp.wav  -c 1 {tempwav_path}ntmp.wav norm -0.1\n",
        "    sox {tempwav_path}ntmp.wav {tempwav_path}ctmp.wav silence 1 0.05 0.1% reverse silence 1 0.05 0.1% reverse\n",
        "    ffmpeg -y -i {tempwav_path}ctmp.wav -c copy -fflags +bitexact -flags:v +bitexact -flags:a +bitexact -ar 22050 {tempwav_path}poop.wav -loglevel error\n",
        "    rm \"$file\"\n",
        "    mv {tempwav_path}poop.wav \"$file\"\n",
        "    rm {tempwav_path}*\n",
        "done\n",
        "''')\n",
        "\n",
        "\n",
        "\n",
        "if audio_processing:\n",
        "  print(f\"\\n\\033[37mNormalización, eliminación de metadatos y comprobación de audios...\")\n",
        "  # Create tempwav directory if it doesn't exist\n",
        "  if not os.path.exists(\"tempwav\"):\n",
        "    os.mkdir(\"tempwav\")\n",
        "  \n",
        "  # run bash audios.sh\n",
        "  os.system(\"bash audios.sh\")\n",
        "\n",
        "totalduration = 0\n",
        "from glob import glob\n",
        "for file_name in glob(wavs_path + \"*.wav\"):\n",
        "    with wave.open(file_name, \"rb\") as wave_file:\n",
        "        frames = wave_file.getnframes()\n",
        "        rate = wave_file.getframerate()\n",
        "        duration = frames / float(rate)\n",
        "        totalduration += duration\n",
        "\n",
        "        if duration >= 12:\n",
        "          print(f\"\\n\\033[33m\\033[1m[AVISO] {file_name} es superior a 12 segundos. La falta de RAM puede\" \n",
        "                \" ¡se producen en un lote de gran tamaño!\")\n",
        "\n",
        "\n",
        "wav_count = len(os.listdir(f\"{wavs_path}\"))\n",
        "print(f\"\\n{wav_count} audios procesados. duración total: {str(datetime.timedelta(seconds=round(totalduration, 0)))}\\n\")\n",
        "\n",
        "#shutil.make_archive(\"/content/processedwavs\", 'zip', '/content/tacotron2/wavs')\n",
        "#files.download('/content/processedwavs.zip')\n",
        "\n",
        "print(\"\\n\\033[32m\\033[1mTodo listo, proceda.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "CHtQAgFfIDJt",
        "outputId": "eb624fd1-8cc3-4843-b4f6-e4ad4f27d6bc"
      },
      "outputs": [],
      "source": [
        "# **Transcripción**,\n",
        "# La transcripción debe ser un archivo ``.TXT`` formateado en <font color=\"red\" size=\"+3\"> ``UTF-8 sin BOM.``\n",
        "\n",
        "os.chdir(tacotron_path)\n",
        "\n",
        "# If txt_path doesn't exist, copy original_txt_path to txt_path\n",
        "shutil.copy(original_txt_path, txt_path)    \n",
        "\n",
        "print(\"\\n\\033[32m\\033[1mTodo listo, proceda.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXC9dLekK14e"
      },
      "source": [
        "##Antes de entrar al paso 5, algunas consideraciones:\n",
        "\n",
        "Si el dataset es grande (es decir muchos audios y minutos - 30 minutos o mas), recomiendo poner en batch size entre 9 y 18.\n",
        "cuidado! ya que puede sobrecargar la memoria del cuaderno y lanzar un error que obligará a comenzar de nuevo. esto es prueba y error. \n",
        "\n",
        "Si el dataset es reducido (15 minutos o menos) recomiendo poner un batch size de entre 3 y 6.\n",
        "\n",
        "El valor Learning Rate varia mucho de dataset en dataset, puede dejarlo como está, o preguntar en nustro servidor de [Discord](https://discord.gg/fakeyou)\n",
        "\n",
        "Si quiere puede experimentar dejando los valores en automáticos marcando las casillas. es posible que de buenos resultados a la primera, como puede que no.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP1trdpN_jV6",
        "outputId": "a28bfc48-d499-4bff-9eae-c8486a791177"
      },
      "outputs": [],
      "source": [
        "# ## **5** Configurar los parámetros del modelo.\n",
        "# ---\n",
        "# ####  Nombre deseado para el modelo\n",
        "Training_file =  txt_path\n",
        "hparams.training_files = Training_file\n",
        "hparams.validation_files = Training_file\n",
        "# hparams to Tune\n",
        "#hparams.use_mmi=True,          # not used in this notebook\n",
        "#hparams.use_gaf=True,          # not used in this notebook\n",
        "#hparams.max_gaf=0.5,           # not used in this notebook\n",
        "#hparams.drop_frame_rate = 0.2  # not used in this notebook\n",
        "hparams.p_attention_dropout=0.1\n",
        "hparams.p_decoder_dropout=0.1\n",
        "\n",
        "hparams.B_ = 8000                   # Decay Rate\n",
        "hparams.C_ = 0                      # Shift learning rate equation by this value\n",
        "hparams.min_learning_rate = 1e-5    # Min Learning Rate\n",
        "\n",
        "# Quality of Life\n",
        "generate_mels = True\n",
        "hparams.show_alignments = True\n",
        "alignment_graph_height = 600\n",
        "alignment_graph_width = 1000\n",
        "\n",
        "# ---\n",
        "# #### Batch size. Marque la casilla si desea que el código calcule el valor óptimo automáticamente\n",
        "auto_batch_size = False \n",
        "hparams.batch_size =  1\n",
        "\n",
        "if auto_batch_size:\n",
        "  import subprocess\n",
        "  gpu_check = subprocess.check_output(\"nvidia-smi -L\", shell=True)\n",
        "  if \"K80\" in str(gpu_check):\n",
        "      hparams.batch_size = 14 if wav_count >= 144 else math.ceil(wav_count / 10.295714)\n",
        "  else:\n",
        "      hparams.batch_size = 18 if wav_count >= 144 else math.ceil(wav_count / 8)\n",
        "  print(f\"batch size ajustado a {hparams.batch_size}\")\n",
        "\n",
        "# ---\n",
        "# #### Calcular automáticamente la tasa de aprendizaje ideal (recomendado)\n",
        "# Learning Rate             # https://www.desmos.com/calculator/ptgcz4vzsw / http://boards.4channel.org/mlp/thread/34778298#p34789030\n",
        "hparams.decay_start = 15000         # wait till decay_start to start decaying learning rate\n",
        "autocalculate_learning_rate = False #@param {type:\"boolean\"}\n",
        "\n",
        "if autocalculate_learning_rate:\n",
        "  hparams.A_ = 0.001*(hparams.batch_size/256)**0.5 # Start/Max Learning Rate\n",
        "  print(f\"Learning Rate definido como {hparams.A_}\")\n",
        "else:\n",
        "  hparams.A_ = 3e-4 #@param [\"3e-6\", \"1e-5\", \"1e-4\", \"5e-4\", \"1e-3\"] {type:\"raw\", allow-input: true}              \n",
        "\n",
        "hparams.load_mel_from_disk = True\n",
        "hparams.text_cleaners=[\"basic_cleaners\"]\n",
        "hparams.ignore_layers=[\"embedding.weight\"]\n",
        "# Layers to reset (None by default, other than foreign languages this param can be ignored)\n",
        "\n",
        "# ---\n",
        "# #### épocas de entrenamiento (no se recomienda cambiar)\n",
        "hparams.epochs =  250#@param {type: \"integer\"}\n",
        "\n",
        "# ---\n",
        "# #### Rango de épocas para guardar (no se recomienda cambiar)\n",
        "saving_interval =  1#@param {type: \"integer\"}\n",
        "\n",
        "torch.backends.cudnn.enabled = hparams.cudnn_enabled\n",
        "torch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n",
        "\n",
        "# ---\n",
        "# #### Dónde guardar tu modelo en tu Drive\n",
        "\n",
        "output_directory = f\"{new_model_path}model1\" #@param {type: \"string\"}\n",
        "log_directory = f\"{new_model_path}logs\" # Location to save Log files locally\n",
        "log_directory2 = None # Location to copy log files (done at the end of each epoch to cut down on I/O)\n",
        "checkpoint_path = output_directory+(r'/')+model_filename\n",
        "\n",
        "# ---\n",
        "# ####¿Utilizar el Warmstart? Si vas a seguir entrenando desde un punto de control, desactívalo.\n",
        "warm_start = False\n",
        "\n",
        "# ---\n",
        "# #### Intervalo de épocas para generar muestras de audio del modelo:\n",
        "sample_interbal =  5\n",
        "# ---\n",
        "# #### guardar muestras de audio del modelo (bueno para enviar los progresos de entrenamiento):\n",
        "save_audios =  True\n",
        "audio_path = output_directory\n",
        "# ---\n",
        "# ####  Elija un modelo base para una alineación más rápida (muy recomendable)\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "model_base = \"spanish-male\" #@param [\"spanish-male\", \"spanish-male-castilian\", \"spanish-female\", \"None\"]\n",
        "ids = {\"spanish-male\": \"1rNwMflvcURMQ5kh8hVJgn458sXj5oehx\", \"spanish-male-castilian\": \"1-1xz_0Bw5Xye1c4GE4kd7WmkNDb6Cm7M\", \"spanish-female\": \"1qr-fyWR1nuAt6K5lMpeXvbY0DWD3cuDg\"}\n",
        "file_status = os.path.isfile(checkpoint_path)\n",
        "if model_base != \"None\":\n",
        "    if file_status == True:\n",
        "      print(\"\\n\\033[33m\\033[1m[Aviso] Ya existe un archivo con ese nombre en tu disco. Si quieres utilizar el modelo base tendrás que borrar/renombrar este archivo\")  \n",
        "    else:\n",
        "      print(f\"Descarga de la plantilla {model_base} en {checkpoint_path}\")\n",
        "      !gdown {ids[model_base]} -O \"$checkpoint_path\"\n",
        "# ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "341f7c045def4a46b9ebe0d67d169aa7",
            "89d8b831ec5b427188e4fa1069409c19",
            "94b77e7c4b9445c0ab588a91acd2434e",
            "eac0816e2d6e44f6a2655ae78eb82ba7",
            "0e6043e766ce4def8c94394d680a837d",
            "bc15f02e756c4f3186d9b8fae304612f",
            "b87c9dde80af4302b35a3aba59aecd1b",
            "12e53a09710042e395f339d01d8e2ea8",
            "5f97550cd8994a7ba072d7617671155c",
            "dcfa033571da4088b46e2e5db5aebe80",
            "791c6ad86d1c4abc88e019fb689a3b76"
          ]
        },
        "id": "A2-xc6EcACWc",
        "outputId": "1f33c991-c49a-4890-f804-776b20e9b0ae"
      },
      "outputs": [],
      "source": [
        "# ## **6** Convierte los WAV en espectrogramas de Mel y comprueba los archivos.\n",
        "from contextlib import redirect_stdout\n",
        "import fileinput\n",
        "\n",
        "\n",
        "with open(original_txt_path, 'r') as f:  \n",
        "    new_text = '\\n'.join([line.strip() for line in f.read().split('\\n') if line.strip()])\n",
        "    with open(original_txt_path, 'w') as n:  \n",
        "      n.write(new_text)\n",
        "\n",
        "f_input = open(original_txt_path,'r')\n",
        "text = f_input.readlines()\n",
        "#print(text)\n",
        "text2 = len(text)\n",
        "dummytext = \"\"\n",
        "#print(text2)\n",
        "for number in range(0,text2):\n",
        " dummytext = number\n",
        " if(str(dummytext) + \".wav\") in (text[number]):\n",
        "   pass\n",
        " else:\n",
        "  dummytext = str(text[number])\n",
        "  dummytext2 = dummytext.split(\"|\")\n",
        "  try:\n",
        "    dummytext2[1] = re.sub(r\"(\\d+)\", lambda x: num2words.num2words(int(x.group(0)), lang='es'), dummytext2[1],)\n",
        "  except IndexError:\n",
        "    replace = dummytext.replace('\\n', '')\n",
        "    raise Exception(f\"esta línea '{replace}' tiene un problema, por favor, compruebe.\")\n",
        "  #print(dummytext2[1])\n",
        "  text[number]  = (str(dummytext2[0] + \"|\" + dummytext2[1].lower()))\n",
        "  #print(text[number])\n",
        "\n",
        "  #print(text[number])\n",
        "textfinal = \" \"\n",
        "for number in range(text2):\n",
        " textfinal += text[number]\n",
        "textfinal = textfinal.replace('[','')\n",
        "textfinal = textfinal.replace(\"',\",'')\n",
        "textfinal = textfinal.replace(\"'\",'')\n",
        "textfinal = textfinal.replace(']','')\n",
        "textfinal = textfinal.replace('\\\\n','\\n')\n",
        "textfinal = textfinal.replace(' wavs','wavs')\n",
        "t = textfinal\n",
        "\"\".join([s for s in t.strip().splitlines(True) if s.strip()])\n",
        "temporal_txt = original_txt_path.replace(\"list.txt\", \"list1.txt\")\n",
        "print(textfinal, file=open(temporal_txt, \"a\"))\n",
        "\n",
        "# Remove original_txt_path and move temporal_txt to original_txt_path\n",
        "os.rename(temporal_txt, original_txt_path)\n",
        "os.remove(temporal_txt)\n",
        "\n",
        "if generate_mels:\n",
        "    create_mels()\n",
        "\n",
        "print(\"Checking for missing files\")\n",
        "# ---- Replace .wav with .npy in filelists ----\n",
        "!sed -i -- 's,.wav|,.npy|,g' {hparams.training_files}; sed -i -- 's,.wav|,.npy|,g' {hparams.validation_files}\n",
        "\n",
        "check_dataset(hparams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614,
          "referenced_widgets": [
            "6eee4514689d418593f5c6cd12fdb98f",
            "6b72bdbfdca7436b8ddf0de2db6bcc56",
            "c0cf2494129947d68ba458aa052f713a",
            "3c7725ca17ff4af688c8a5269d7ad34c",
            "c45e0f4966444bdf8ff76938ffa8217a",
            "4b136add1e314f5884509cbd86ea93b2",
            "4987ebabbc844974b5710477d9e953ba",
            "39048d1d9920497ea25c0e2c15627609",
            "fb64450c408e4d01b24688bb15ebe863",
            "8111322f270641f49f38cf1dd15c9ac0",
            "da7101ec5d404952a4638f7eb2c9c553",
            "7641c72c810e4407804603a40774ccfd",
            "7be2cc8533704c5e94e59983a65ffb5b",
            "578aca5f483947d399c35c934a2bfd7e",
            "b5629438b5b4446da18d64c250c4d11d",
            "cd61d6950c754bbaa3c1a8d6d768df20",
            "b5bf0cb9faa64355bec57529e678b76f",
            "5ac5ff519071462f8c97c95d499b531c",
            "740a84744e4c4a0494ef3d1a5855adf6",
            "f623273c22d242ce9121ccb8da2eae81",
            "a4f8c9736e2541beb6ddc12422f86d4c",
            "6ef3509d85034798858857eb8a464e18"
          ]
        },
        "id": "iqIk1dIuCqhx",
        "outputId": "3e17b54a-bb4b-4046-965e-816220a7777d"
      },
      "outputs": [],
      "source": [
        "# # **7** Empezar a entrenar (por fin).\n",
        "# ---\n",
        "# #### Texto en el que se generarán las muestras de audio del modelo. Un texto muy grande puede llevar mucho tiempo y ser difícil de generar, y un texto muy pequeño puede no demostrar el modelo muy bien, así que elige sabiamente.\n",
        "sampletext = \"este es un texto de prueba para controlar como suena el modelo\" #@param {type: \"string\"}\n",
        "print('FP16 Run:', hparams.fp16_run)\n",
        "print('Dynamic Loss Scaling:', hparams.dynamic_loss_scaling)\n",
        "print('Distributed Run:', hparams.distributed_run)\n",
        "print('cuDNN Enabled:', hparams.cudnn_enabled)\n",
        "print('cuDNN Benchmark:', hparams.cudnn_benchmark)\n",
        "train(output_directory, log_directory, checkpoint_path,\n",
        "      warm_start, n_gpus, rank, group_name, hparams, log_directory2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5KV1so1ni_R"
      },
      "source": [
        "Si salta un error \"CUDA out of memory\", ve a \"Entorno de ejecución\" > \"Reiniciar entorno de ejecución\"\n",
        "\n",
        "vuelve al paso 2 y ejecutalo nuevamente.\n",
        "\n",
        "Luego vuelve al paso 5, disminuye el batch size y ejecutalo nuevamente. \n",
        "\n",
        "Finalmente, ejecuta el paso 7 nuevamente para volver a entrenar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4ql3OWWPKZv"
      },
      "source": [
        "# **Notas finales**\n",
        "\n",
        "## Un buen entrenamiento se ve así aquí:\n",
        "![img.png](https://media.discordapp.net/attachments/835971020569051216/851469553355587614/download_2.png)\n",
        "\n",
        "###si se les vence la sesion del cuaderno cuando estan entrenando, y en su cuenta indica que no hay mas GPU disponible, y los hacen esperar. vayan al directorio del modelo en su drive, lo descargan y lo suben a otra cuenta de drive que tengan (en los mismos directorios: collab/outdir), luego inician el cuaderno nuevamente con esa otra cuenta, cargando todos los mismos archivos (audio, lista y los detalles del modelo, mismo nombre etc...) EN EL PASO 5 DESMARCAN LA CASILLA warm_start ANTES DE EJECUTARLO. Todo esto sirve para poder entrenar desde el punto de control dejado en la anterior cuenta. asi se ahorran esperas entre entrenamiento y entrenamiento.\n",
        "\n",
        "Links útiles:\n",
        "\n",
        "[Tutorial de clonación](https://docs.google.com/document/d/1Tisx9RE119z9ULD6XPFZAg8sDYEgTcjlYgpjotEqiao/edit?usp=sharing)\n",
        "\n",
        "[Cuaderno de transcripciones](https://colab.research.google.com/drive/179oopKDixj00hjkJmtQ-UWTYZ4xT9cZj#scrollTo=v5U0FgCBfyuY)\n",
        "\n",
        "[Cuaderno de separación / aislamiento vocal](https://colab.research.google.com/drive/1FHR7s81NwmXaW9ze7yjvGORmxneayeUU?usp=sharing)\n",
        "\n",
        "[Cuaderno de entrenamiento](https://colab.research.google.com/drive/1ZaqeznQYf7ncmN4nYU4-WoJLZxVbCv60#scrollTo=h4ql3OWWPKZv)\n",
        "\n",
        "[Cuaderno de síntesis](https://colab.research.google.com/drive/1_qAwEaT5rlF3XrxVQq1m2bdYurTc7OyZ?usp=sharing)\n",
        "\n",
        "[Página de FakeYou](https://fakeyou.com/)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e6043e766ce4def8c94394d680a837d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12e53a09710042e395f339d01d8e2ea8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "341f7c045def4a46b9ebe0d67d169aa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89d8b831ec5b427188e4fa1069409c19",
              "IPY_MODEL_94b77e7c4b9445c0ab588a91acd2434e",
              "IPY_MODEL_eac0816e2d6e44f6a2655ae78eb82ba7"
            ],
            "layout": "IPY_MODEL_0e6043e766ce4def8c94394d680a837d"
          }
        },
        "39048d1d9920497ea25c0e2c15627609": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c7725ca17ff4af688c8a5269d7ad34c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8111322f270641f49f38cf1dd15c9ac0",
            "placeholder": "​",
            "style": "IPY_MODEL_da7101ec5d404952a4638f7eb2c9c553",
            "value": " 0/250 [00:02&lt;?, ?it/s]"
          }
        },
        "4987ebabbc844974b5710477d9e953ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b136add1e314f5884509cbd86ea93b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "578aca5f483947d399c35c934a2bfd7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_740a84744e4c4a0494ef3d1a5855adf6",
            "max": 54,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f623273c22d242ce9121ccb8da2eae81",
            "value": 0
          }
        },
        "5ac5ff519071462f8c97c95d499b531c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f97550cd8994a7ba072d7617671155c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b72bdbfdca7436b8ddf0de2db6bcc56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b136add1e314f5884509cbd86ea93b2",
            "placeholder": "​",
            "style": "IPY_MODEL_4987ebabbc844974b5710477d9e953ba",
            "value": "  0%"
          }
        },
        "6eee4514689d418593f5c6cd12fdb98f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b72bdbfdca7436b8ddf0de2db6bcc56",
              "IPY_MODEL_c0cf2494129947d68ba458aa052f713a",
              "IPY_MODEL_3c7725ca17ff4af688c8a5269d7ad34c"
            ],
            "layout": "IPY_MODEL_c45e0f4966444bdf8ff76938ffa8217a"
          }
        },
        "6ef3509d85034798858857eb8a464e18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "740a84744e4c4a0494ef3d1a5855adf6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7641c72c810e4407804603a40774ccfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7be2cc8533704c5e94e59983a65ffb5b",
              "IPY_MODEL_578aca5f483947d399c35c934a2bfd7e",
              "IPY_MODEL_b5629438b5b4446da18d64c250c4d11d"
            ],
            "layout": "IPY_MODEL_cd61d6950c754bbaa3c1a8d6d768df20"
          }
        },
        "791c6ad86d1c4abc88e019fb689a3b76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7be2cc8533704c5e94e59983a65ffb5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5bf0cb9faa64355bec57529e678b76f",
            "placeholder": "​",
            "style": "IPY_MODEL_5ac5ff519071462f8c97c95d499b531c",
            "value": "  0%"
          }
        },
        "8111322f270641f49f38cf1dd15c9ac0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89d8b831ec5b427188e4fa1069409c19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc15f02e756c4f3186d9b8fae304612f",
            "placeholder": "​",
            "style": "IPY_MODEL_b87c9dde80af4302b35a3aba59aecd1b",
            "value": "100%"
          }
        },
        "94b77e7c4b9445c0ab588a91acd2434e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12e53a09710042e395f339d01d8e2ea8",
            "max": 540,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f97550cd8994a7ba072d7617671155c",
            "value": 540
          }
        },
        "a4f8c9736e2541beb6ddc12422f86d4c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5629438b5b4446da18d64c250c4d11d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4f8c9736e2541beb6ddc12422f86d4c",
            "placeholder": "​",
            "style": "IPY_MODEL_6ef3509d85034798858857eb8a464e18",
            "value": " 0/54 [00:02&lt;?, ?it/s]"
          }
        },
        "b5bf0cb9faa64355bec57529e678b76f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b87c9dde80af4302b35a3aba59aecd1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc15f02e756c4f3186d9b8fae304612f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0cf2494129947d68ba458aa052f713a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39048d1d9920497ea25c0e2c15627609",
            "max": 250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb64450c408e4d01b24688bb15ebe863",
            "value": 0
          }
        },
        "c45e0f4966444bdf8ff76938ffa8217a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd61d6950c754bbaa3c1a8d6d768df20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da7101ec5d404952a4638f7eb2c9c553": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcfa033571da4088b46e2e5db5aebe80": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eac0816e2d6e44f6a2655ae78eb82ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcfa033571da4088b46e2e5db5aebe80",
            "placeholder": "​",
            "style": "IPY_MODEL_791c6ad86d1c4abc88e019fb689a3b76",
            "value": " 540/540 [00:09&lt;00:00, 63.61it/s]"
          }
        },
        "f623273c22d242ce9121ccb8da2eae81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb64450c408e4d01b24688bb15ebe863": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
